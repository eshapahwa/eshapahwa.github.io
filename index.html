<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Esha Pahwa</title>

    <meta name="author" content="Jon Barron">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Esha Pahwa
                </p>
                <p align="justify">I am a Member of Technical Staff at <a href="https://www.adobe.com/">Adobe</a>, where I work on integrating Generative AI with Adobe Campain products.
                </p>
                <p align="justify">
                  In early 2023, I had the privilege of joining <a href="https://research.google/locations/bangalore/">Google Research 
                  India</a> as a Research Associate in the Shopping Ads team, where, mentored by
                  <a href="https://www.linkedin.com/in/gausri108">Gaurav Srivastava</a> and 
                  <a href="https://www.prateekjain.org/">Prateek Jain</a>. 
                  I focused on advancing product retrieval techniques using the 
                  <a href="https://jax.readthedocs.io/en/latest/notebooks/quickstart.html">JAX</a> 
                  framework for my Bachelor's thesis. In 2022, I embarked on a remarkable
                  journey of learning and contribution. As a summer research intern at the 
                  <a href="https://research.adobe.com/">Media and Data Science Research Labs 
                  (Adobe Research)</a>, I collaborated with <a href="https://scholar.google.com/citations?user=n8iUBg8AAAAJ">Balaji Krishnamurthy</a>, 
                  focusing on <a href="https://www.lotame.com/back-basics-lookalike-modeling/">lookalike modeling</a> 
                  and segment prediction by leveraging second-party data from diverse brands. The same year, 
                  I was among the six women across India awarded the <a href="https://research.adobe.com/adobe-india-women-in-technology-scholarship/">Adobe Women in Tech Scholarship</a>. 
                  This was also the year I explored solutions to the <a href="https://www.geeksforgeeks.org/modal-collapse-in-gans/">mode collapse</a> problem in GANs 
                  during my first Bachelor's thesis as a visiting researcher at <a href="http://www.cvc.uab.es/">CVC, Barcelona</a> 
                  in collaboration with <a href="http://www.lherranz.org/">Prof. Luis Herranz</a>.
                  Prior to that, I was honored to receive the <a href="https://ghc.anitab.org/">Grace Hopper Celebration Scholarship</a> in 2021
                  and the prestigious research fellowship by <a href="https://artpark.iisc.ac.in/">ARTPARK IISc</a> in 2022, 
                  enabling me to collaborate with <a href="https://scholar.google.com/citations?hl=en&user=YyCGRO8AAAAJ">Prof. Pratik Narang</a>
                  on image restoration projects throughout the year, contributing significantly to the field of image processing and restoration. 
                  I have also worked in the field of super-resolution during an internship with the 
                  <a href="https://vcg.seas.harvard.edu/">VCG group</a> at Harvard University, 
                  guided by <a href="https://en.wikipedia.org/wiki/Hanspeter_Pfister">Prof. Hanspeter Pfister</a> 
                  and mentor <a href="https://sites.google.com/view/salma-abdelmagid/">Salma Abdel Magid</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:pahwa.esha@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/EshaPahwa_resume_final.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=mnUQ4JwAAAAJ&hl=en&authuser=1">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/esha-pahwa/">Linkedin</a> &nbsp;/&nbsp;
                  <a href="https://github.com/eshapahwa">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/EshaPahwa_pic.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/EshaPahwa_pic_circle.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in computer vision, natural language processing, and machine learning. Much of my research is based on the applications of these topics in real-world scenarios.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/lvrnet.png" alt="Boundary_png" style="border-style: none" width="250" height="180">
              </td>
              <td width="75%" valign="middle">
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/27007">
                  <span class="papertitle">LVRNet: Lightweight Image Restoration for Aerial Images under Low Visibility (Student Abstract)</span>
                </a>
                <br>
                <strong>Esha Pahwa*</strong>,
                <a href="https://achleshwar.github.io/">Achleshwar Luthra*</a>
                <a href="https://scholar.google.com/citations?hl=en&user=YyCGRO8AAAAJ">Pratik Narang</a>
                <br>
                <em>AAAI 2023</em> <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://arxiv.org/pdf/2301.05434.pdf">paper</a> / <a href="https://eshapahwa.github.io/lvrnet.github.io/">project page</a> / <a href="https://drive.google.com/file/d/1lQUzXrsCRERWxrP9qUomC_-R_rfD92-t/view?usp=sharing">poster</a> / <a href="https://docs.google.com/presentation/d/1jRg8nC07neuUNMKA9Iq62k9DVo-yGxUIzDsVMeecQMc/edit?pli=1#slide=id.g1c4a3b21676_2_45">slides</a> / <a href="https://github.com/Achleshwar/lvrnet">code</a> / <a href="data/lvrnet.bib">bibtex</a>
                <p>We generate the LowVis-AFO dataset, containing 3647 paired dark-hazy and clear images. We also introduce a new lightweight deep learning model called Low-Visibility Restoration Network (LVRNet)</p>
              </td>
            </tr>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/swta.png" alt="Boundary_png" style="border-style: none" width="250" height="180">
              </td>
              <td width="75%" valign="middle">
                <a href="https://ieeexplore.ieee.org/abstract/document/10191750">
                  <span class="papertitle">SWTA: Sparse weighted temporal attention for drone-based activity recognition</span>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=7wgb6VAAAAAJ&hl=en&authuser=1">Santosh Kumar Yadav</a>,
                <strong>Esha Pahwa</strong>,
                <a href="https://achleshwar.github.io/">Achleshwar Luthra</a>,
                <a href="https://scholar.google.com/citations?user=BtURBxsAAAAJ&hl=en&authuser=1">Kamlesh Tiwari</a>,
                <a href="https://scholar.google.com/citations?user=3iuT3pcAAAAJ&hl=en&authuser=1">Hari Mohan Pandey</a>
                <br>
                <em>International Joint Conference on Neural Networks (IJCNN)</em>, 2023
                <br>
                <a href="https://eprints.bournemouth.ac.uk/38505/1/Drone_IJCNN%20%282%29.pdf">paper</a> / <a href="data/swta.bib">bibtex</a>
                <p>We propose a novel Sparse Weighted Temporal Attention (SWTA) module to utilize sparsely sampled video frames for obtaining global weighted temporal attention. The SWTA network can be used as a plug-in module to the existing deep CNN architectures, for optimizing them to learn temporal information by eliminating the need for a separate temporal stream.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/droneatten.png" alt="Boundary_png" style="border-style: none" width="250" height="180">
              </td>
              <td width="75%" valign="middle">
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S089360802200497X">
                  <span class="papertitle">DroneAttention: Sparse weighted temporal attention for drone-camera based activity recognition</span>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=7wgb6VAAAAAJ&hl=en&authuser=1">Santosh Kumar Yadav</a>,
                <a href="https://achleshwar.github.io/">Achleshwar Luthra</a>,
                <strong>Esha Pahwa</strong>,
                <a href="https://scholar.google.com/citations?user=BtURBxsAAAAJ&hl=en&authuser=1">Kamlesh Tiwari</a>,
                <a href="https://scholar.google.com/citations?user=YQymeAEAAAAJ&hl=en&authuser=1">Heena Rathore</a>,
                <a href="https://scholar.google.com/citations?user=3iuT3pcAAAAJ&hl=en&authuser=1">Hari Mohan Pandey</a>,
                <a href="https://scholar.google.com/citations?user=J6YWBB4AAAAJ&hl=en&authuser=1">Peter Corcoran</a>
                <br>
                <em>Neural Networks 2023</em> (Journal Paper) 
                <br>
                <a href="https://arxiv.org/ftp/arxiv/papers/2212/2212.03384.pdf">paper</a> / <a href="data/droneatten.bib">bibtex</a>
                <p>This is the extended version of the study conducted for the proposed SWTA network. It contains a detailed survey of the existing approaches with an elaborate explanation of the SWTA network and its components.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/tfnet.png" alt="Boundary_png" style="border-style: none" width="250" height="180">
              </td>
              <td width="75%" valign="middle">
                <a href="https://ieeexplore.ieee.org/abstract/document/9897548/">
                  <span class="papertitle">Conditional RGB-T Fusion for Effective Crowd Counting</span>
                </a>
                <br>
                <strong>Esha Pahwa*</strong>,
                <a href="https://achleshwar.github.io/">Achleshwar Luthra*</a>,
                <a href="https://scholar.google.com/citations?user=aRU_qYYAAAAJ&hl=en&authuser=1">Sanjeet Kapadia*</a>,
                <a href="https://scholar.google.com/citations?user=ixEh0nEAAAAJ&hl=en&authuser=1">Shreyas Sheeranali*</a>
                <br>
                <em>IEEE ICIP 2022</em> <font color="red"><strong>(Poster Presentation)</strong></font>
                <br>
                <a href="https://www.academia.edu/download/96398948/09897548.pdf">paper</a> / <a href="https://drive.google.com/file/d/1ejbKPTW70N9367xAHYOddEBsPuNpw7Ze/view?usp=sharing">poster</a> / <a href="https://drive.google.com/file/d/1VFBqexpCEPis2CehrCV41ayr1Tw-APho/view?usp=sharing">talk</a> / <a href="https://github.com/ShreyasSR/TFNet">code</a> / <a href="data/tfnet.bib">bibtex</a>
                <p>We introduce a novel architecture Toggle-Fusion Network (TFNet) that effectively utilises a multimodal dataset, RGBT-CC, containing pairs of thermal and RGB images. </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/medskip.png" alt="Boundary_png" style="border-style: none" width="250" height="180">
              </td>
              <td width="75%" valign="middle">
                <a href="https://openaccess.thecvf.com/content/ICCV2021W/CVAMD/html/Pahwa_MedSkip_Medical_Report_Generation_Using_Skip_Connections_and_Integrated_Attention_ICCVW_2021_paper.html">
                  <span class="papertitle">Medskip: Medical report generation using skip connections and integrated attention</span>
                </a>
                <br>
                <strong>Esha Pahwa*</strong>,
                <a href="https://www.linkedin.com/in/dwij-mehta/?originalSubdomain=in">Dwij Mehta*</a>,
                <a href="https://scholar.google.com/citations?user=aRU_qYYAAAAJ&hl=en&authuser=1">Sanjeet Kapadia*</a>,
                <a href="https://scholar.google.com/citations?user=PwdervMAAAAJ&hl=en&authuser=1">Devansh Jain*</a>
                <a href="https://achleshwar.github.io/">Achleshwar Luthra</a>
                <br>
                <em>ICCV: CVAMD Workshop 2021</em> <font color="red"><strong>(Poster Presentation)</strong></font>
                <br>
                <a href="https://openaccess.thecvf.com/content/ICCV2021W/CVAMD/papers/Pahwa_MedSkip_Medical_Report_Generation_Using_Skip_Connections_and_Integrated_Attention_ICCVW_2021_paper.pdf">paper</a> / <a href="https://drive.google.com/file/d/14NFrnDMYHp3JPr-WcUC6W2UwWcjNTBn7/view?usp=sharing">poster</a> / <a href="data/medskip.bib">bibtex</a>
                <p>We propose a novel architecture of a modified HRNet which includes added skip connections along with convolutional block attention modules (CBAM). We evaluate
                  our model on two publicly available datasets, PEIR Gross and IU X-Ray.</p>
              </td>
            </tr>

          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
              <td width="75%" valign="center">
                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
          <br>
                <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                <br>
                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member, CVPR 2021</a>
                <br>
                <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                <br>
                <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/cs188.jpg" alt="cs188">
              </td>
              <td width="75%" valign="center">
                <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
                <br>
                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
                <br>
                <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
              </td>
            </tr>
            

            <tr>
              <td align="center" style="padding:20px;width:25%;vertical-align:middle">
                <h2>Basically <br> Blog Posts</h2>
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr>
            
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
